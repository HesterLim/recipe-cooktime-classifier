{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv_2 = \"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_test.csv\"\n",
    "train_csv_2 = \"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_train.csv\"\n",
    "\n",
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv_2)\n",
    "train_df = pd.read_csv(train_csv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[['n_steps', 'n_ingredients']]\n",
    "y = train_df['duration_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X_train_name = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_name_vec.npz')\n",
    "X_test_name = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_name_vec.npz')\n",
    "\n",
    "X_train_step = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_steps_vec.npz')\n",
    "X_test_step = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_steps_vec.npz')\n",
    "\n",
    "X_train_ingr = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_ingr_vec.npz')\n",
    "X_test_ingr = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_ingr_vec.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features with CountVec and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'name'\n",
    "k = 1000\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "name_train_mi = mi.fit_transform(X_train_name,y)\n",
    "name_test_mi = mi.transform(X_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'steps'\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "step_train_mi = mi.fit_transform(X_train_step,y)\n",
    "step_test_mi = mi.transform(X_test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'steps'\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "ingr_train_mi = mi.fit_transform(X_train_ingr,y)\n",
    "ingr_test_mi = mi.transform(X_test_ingr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Features for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = train_df.copy()\n",
    "new_train_df = new_train_df[['n_steps','n_ingredients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_matrix = name_train_mi.todense()\n",
    "name_list = name_matrix.tolist()\n",
    "name_df = pd.DataFrame(name_list)\n",
    "name_df = name_df.add_prefix('name_')\n",
    "\n",
    "step_matrix = step_train_mi.todense()\n",
    "step_list = step_matrix.tolist()\n",
    "step_df = pd.DataFrame(step_list)\n",
    "step_df = step_df.add_prefix('step_')\n",
    "\n",
    "ingr_matrix = ingr_train_mi.todense()\n",
    "ingr_list = ingr_matrix.tolist()\n",
    "ingr_df = pd.DataFrame(ingr_list)\n",
    "ingr_df = ingr_df.add_prefix('ingr_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = new_train_df.join(name_df)\n",
    "features_train = features_train.join(step_df)\n",
    "features_train = features_train.join(ingr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = test_df.copy()\n",
    "new_test_df = new_test_df[['n_steps','n_ingredients']]\n",
    "\n",
    "name_matrix_test = name_test_mi.todense()\n",
    "name_list_test = name_matrix_test.tolist()\n",
    "name_df_test = pd.DataFrame(name_list_test)\n",
    "name_df_test = name_df_test.add_prefix('name_')\n",
    "\n",
    "step_matrix_test = step_test_mi.todense()\n",
    "step_list_test = step_matrix_test.tolist()\n",
    "step_df_test = pd.DataFrame(step_list_test)\n",
    "step_df_test = step_df_test.add_prefix('step_')\n",
    "\n",
    "ingr_matrix_test = ingr_test_mi.todense()\n",
    "ingr_list_test = ingr_matrix_test.tolist()\n",
    "ingr_df_test = pd.DataFrame(ingr_list_test)\n",
    "ingr_df_test = ingr_df_test.add_prefix('ingr_')\n",
    "\n",
    "features_test = new_test_df.join(name_df_test)\n",
    "features_test = features_test.join(step_df_test)\n",
    "features_test = features_test.join(ingr_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Dataset \n",
    "train_name_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_name_doc2vec100.csv\", header=None)\n",
    "test_name_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_name_doc2vec100.csv\", header=None)\n",
    "train_steps_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_steps_doc2vec100.csv\", header=None)\n",
    "test_steps_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_steps_doc2vec100.csv\", header=None)\n",
    "train_ingr_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_ingr_doc2vec100.csv\", header=None)\n",
    "test_ingr_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_ingr_doc2vec100.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_doc100 = train_name_doc100.add_suffix('name_doc2vec')\n",
    "feature_train_doc100 = features_train.join(train_name_doc100)\n",
    "train_steps_doc100 = train_steps_doc100.add_suffix('step_doc2vec')\n",
    "feature_train_doc100 = feature_train_doc100.join(train_steps_doc100)\n",
    "train_ingr_doc100 = train_ingr_doc100.add_suffix('ingr_doc2vec')\n",
    "feature_train_doc100 = feature_train_doc100.join(train_ingr_doc100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_doc100 = test_name_doc100.add_suffix('name_doc2vec')\n",
    "feature_test_doc100 = features_test.join(test_name_doc100)\n",
    "test_steps_doc100 = test_steps_doc100.add_suffix('step_doc2vec')\n",
    "feature_test_doc100 = feature_test_doc100.join(test_steps_doc100)\n",
    "test_ingr_doc100 = test_ingr_doc100.add_suffix('ingr_doc2vec')\n",
    "feature_test_doc100 = feature_test_doc100.join(test_ingr_doc100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_name_doc2vec50.csv\", header=None)\n",
    "test_name_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_name_doc2vec50.csv\", header=None)\n",
    "train_steps_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_steps_doc2vec50.csv\", header=None)\n",
    "test_steps_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_steps_doc2vec50.csv\", header=None)\n",
    "train_ingr_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_ingr_doc2vec50.csv\", header=None)\n",
    "test_ingr_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_ingr_doc2vec50.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_tot = train_name_tot.add_suffix('name_doc2vec_50')\n",
    "feature_train_tot = features_train.join(train_name_tot)\n",
    "train_steps_tot = train_steps_tot.add_suffix('step_doc2vec_50')\n",
    "feature_train_tot = feature_train_tot.join(train_steps_tot)\n",
    "train_ingr_tot = train_ingr_tot.add_suffix('ingr_doc2vec_50')\n",
    "feature_train_tot = feature_train_tot.join(train_ingr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_tot = test_name_tot.add_suffix('name_doc2vec_50')\n",
    "feature_test_tot = features_test.join(test_name_tot)\n",
    "test_steps_tot = test_steps_tot.add_suffix('step_doc2vec_50')\n",
    "feature_test_tot = feature_test_tot.join(test_steps_tot)\n",
    "test_ingr_tot = test_ingr_tot.add_suffix('ingr_doc2vec_50')\n",
    "feature_test_tot = feature_test_tot.join(test_ingr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "X_train_tot_split, X_test_tot_split, y_train_tot_split, y_test_tot_split = train_test_split(feature_train_tot, y, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=0.01, solver='sag')\n",
    "lgr.fit(feature_train_tot, y)\n",
    "ybar = lgr.predict(feature_test_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8253030303030303\n",
      "f1 score: 0.8253030303030303\n"
     ]
    }
   ],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)\n",
    "acc = lgr.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_tot_split)\n",
    "print(\"f1 score:\", f1_score(y_test_tot_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features with Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "k=1000\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train,y)\n",
    "X_train_x2 = x2.transform(X_train)\n",
    "X_test_x2 = x2.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features with Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 28138 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=1000\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train,y)\n",
    "X_train_mi = mi.transform(X_train)\n",
    "X_test_mi = mi.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Holdout or Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split,y_train_split)\n",
    "acc = lgr.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(join_train_df, y_train)\n",
    "acc = np.mean(cross_val_score(lgr, join_train_df, y_train, cv=5))\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both Holdout and Cross-validation get similar accuracy score. However, holdout is significantly faster than cross validation and for this reason, holdout strategy will be chosen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB Accuracy: 0.36984848484848487\n",
      "5-nearest neighbour Accuracy: 0.7217424242424243\n",
      "Decision Tree Accuracy: 0.7133333333333334\n",
      "One_R Accuracy: 0.6552272727272728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "models = [GaussianNB(),\n",
    "          #MultinomialNB(),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "titles = ['GNB',\n",
    "          #'MNB',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree',\n",
    "          'One_R']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(X_train_tot_split,y_train_tot_split)\n",
    "    acc = model.score(X_test_tot_split,y_test_tot_split)\n",
    "    print(title, \"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking, Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(), KNeighborsClassifier(), MultinomialNB()]\n",
    "titles = ['Decision Tree', 'KNeighborsClassifier()', 'Multinomial NB']\n",
    "\n",
    "\n",
    "\n",
    "meta_classifier_lr = LogisticRegression()\n",
    "stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker_lr.fit(features_train, y)\n",
    "\n",
    "ybar = stacker_lr.predict(features_test)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)\n",
    "# Kaggle score 0.63666 Test Stacking with KNN\n",
    "# Kaggle score 0.73733 Test Staking without KNN\n",
    "# Kaggle score 0.68333 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "\n",
    "Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n",
    "\n",
    "Third, logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n",
    "\n",
    "Fourth, logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n",
    "\n",
    "Finally, logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced') # inverse class weighting to reduce bias\n",
    "lg2.fit(X_train_split,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing features - binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_split)\n",
    "X_train_scaled = scaler.transform(X_train_split)\n",
    "X_test_scaled = scaler.transform(X_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_scaled,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_scaled , y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does Scaling improve performance?  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_scaled,y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = lgr.score(X_test_scaled , y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling does not seem to improve the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Reducing Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third, logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance \n",
    "pca = PCA(n_components=0.99, whiten = True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Show results \n",
    "print(\"Original number of features:\", X_train_scaled.shape[1])\n",
    "print(\"Reduced number of features:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct PCA from fitted set\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does PCA improve performance? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_pca,y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = lgr.score(X_test_pca , y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance + reduce correlation? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_pca,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_pca, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_tot_split, y_train_tot_split)\n",
    "acc = lgr.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "### HIGHEST KAGGLE SCORE 0.81333\n",
    "lgr = LogisticRegression(C=0.01, solver='sag')\n",
    "lgr.fit(feature_train_tot, y)\n",
    "ybar = lgr.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sag_tot_0_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(C=0.1, solver='sag')\n",
    "lgr.fit(X_train_tot_split, y_train_tot_split)\n",
    "acc = lgr.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=0.001, solver='sag')\n",
    "lgr.fit(X_train_tot_split, y_train_tot_split)\n",
    "ybar = lgr.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sag_split_0_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(C=1.0, solver='newton-cg')\n",
    "lgr.fit(feature_train_tot, y)\n",
    "ybar = lgr.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('newt_tot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(C=1.0, solver='saga')\n",
    "lgr.fit(X_train_tot_split, y_train_tot_split)\n",
    "ybar = lgr.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sag_split.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
