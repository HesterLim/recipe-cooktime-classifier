{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv_2 = \"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_test.csv\"\n",
    "train_csv_2 = \"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_train.csv\"\n",
    "\n",
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv_2)\n",
    "train_df = pd.read_csv(train_csv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['n_steps', 'n_ingredients']]\n",
    "y = train_df['duration_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X_train_name = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_name_vec.npz')\n",
    "X_test_name = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_name_vec.npz')\n",
    "\n",
    "X_train_step = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_steps_vec.npz')\n",
    "X_test_step = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_steps_vec.npz')\n",
    "\n",
    "X_train_ingr = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_ingr_vec.npz')\n",
    "X_test_ingr = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_ingr_vec.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features with CountVec and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'name'\n",
    "k = 1000\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "name_train_mi = mi.fit_transform(X_train_name,y)\n",
    "name_test_mi = mi.transform(X_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'steps'\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "step_train_mi = mi.fit_transform(X_train_step,y)\n",
    "step_test_mi = mi.transform(X_test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'steps'\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "ingr_train_mi = mi.fit_transform(X_train_ingr,y)\n",
    "ingr_test_mi = mi.transform(X_test_ingr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Features for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = train_df.copy()\n",
    "new_train_df = new_train_df[['n_steps','n_ingredients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_matrix = name_train_mi.todense()\n",
    "name_list = name_matrix.tolist()\n",
    "name_df = pd.DataFrame(name_list)\n",
    "name_df = name_df.add_prefix('name_')\n",
    "\n",
    "step_matrix = step_train_mi.todense()\n",
    "step_list = step_matrix.tolist()\n",
    "step_df = pd.DataFrame(step_list)\n",
    "step_df = step_df.add_prefix('step_')\n",
    "\n",
    "ingr_matrix = ingr_train_mi.todense()\n",
    "ingr_list = ingr_matrix.tolist()\n",
    "ingr_df = pd.DataFrame(ingr_list)\n",
    "ingr_df = ingr_df.add_prefix('ingr_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = new_train_df.join(name_df)\n",
    "features_train = features_train.join(step_df)\n",
    "features_train = features_train.join(ingr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = test_df.copy()\n",
    "new_test_df = new_test_df[['n_steps','n_ingredients']]\n",
    "\n",
    "name_matrix_test = name_test_mi.todense()\n",
    "name_list_test = name_matrix_test.tolist()\n",
    "name_df_test = pd.DataFrame(name_list_test)\n",
    "name_df_test = name_df_test.add_prefix('name_')\n",
    "\n",
    "step_matrix_test = step_test_mi.todense()\n",
    "step_list_test = step_matrix_test.tolist()\n",
    "step_df_test = pd.DataFrame(step_list_test)\n",
    "step_df_test = step_df_test.add_prefix('step_')\n",
    "\n",
    "ingr_matrix_test = ingr_test_mi.todense()\n",
    "ingr_list_test = ingr_matrix_test.tolist()\n",
    "ingr_df_test = pd.DataFrame(ingr_list_test)\n",
    "ingr_df_test = ingr_df_test.add_prefix('ingr_')\n",
    "\n",
    "features_test = new_test_df.join(name_df_test)\n",
    "features_test = features_test.join(step_df_test)\n",
    "features_test = features_test.join(ingr_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Dataset \n",
    "train_name_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_name_doc2vec100.csv\", header=None)\n",
    "test_name_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_name_doc2vec100.csv\", header=None)\n",
    "train_steps_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_steps_doc2vec100.csv\", header=None)\n",
    "test_steps_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_steps_doc2vec100.csv\", header=None)\n",
    "train_ingr_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_ingr_doc2vec100.csv\", header=None)\n",
    "test_ingr_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_ingr_doc2vec100.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_doc100 = train_name_doc100.add_suffix('name_doc2vec')\n",
    "feature_train_doc100 = features_train.join(train_name_doc100)\n",
    "train_steps_doc100 = train_steps_doc100.add_suffix('step_doc2vec')\n",
    "feature_train_doc100 = feature_train_doc100.join(train_steps_doc100)\n",
    "train_ingr_doc100 = train_ingr_doc100.add_suffix('ingr_doc2vec')\n",
    "feature_train_doc100 = feature_train_doc100.join(train_ingr_doc100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_doc100 = test_name_doc100.add_suffix('name_doc2vec')\n",
    "feature_test_doc100 = features_test.join(test_name_doc100)\n",
    "test_steps_doc100 = test_steps_doc100.add_suffix('step_doc2vec')\n",
    "feature_test_doc100 = feature_test_doc100.join(test_steps_doc100)\n",
    "test_ingr_doc100 = test_ingr_doc100.add_suffix('ingr_doc2vec')\n",
    "feature_test_doc100 = feature_test_doc100.join(test_ingr_doc100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_name_doc2vec50.csv\", header=None)\n",
    "test_name_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_name_doc2vec50.csv\", header=None)\n",
    "train_steps_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_steps_doc2vec50.csv\", header=None)\n",
    "test_steps_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_steps_doc2vec50.csv\", header=None)\n",
    "train_ingr_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_ingr_doc2vec50.csv\", header=None)\n",
    "test_ingr_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_ingr_doc2vec50.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_tot = train_name_tot.add_suffix('name_doc2vec_50')\n",
    "feature_train_tot = features_train.join(train_name_tot)\n",
    "train_steps_tot = train_steps_tot.add_suffix('step_doc2vec_50')\n",
    "feature_train_tot = feature_train_tot.join(train_steps_tot)\n",
    "train_ingr_tot = train_ingr_tot.add_suffix('ingr_doc2vec_50')\n",
    "feature_train_tot = feature_train_tot.join(train_ingr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_tot = test_name_tot.add_suffix('name_doc2vec_50')\n",
    "feature_test_tot = features_test.join(test_name_tot)\n",
    "test_steps_tot = test_steps_tot.add_suffix('step_doc2vec_50')\n",
    "feature_test_tot = feature_test_tot.join(test_steps_tot)\n",
    "test_ingr_tot = test_ingr_tot.add_suffix('ingr_doc2vec_50')\n",
    "feature_test_tot = feature_test_tot.join(test_ingr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "X_train_tot_split, X_test_tot_split, y_train_tot_split, y_test_tot_split = train_test_split(feature_train_tot, y, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=0.01, solver='sag')\n",
    "lgr.fit(feature_train_tot, y)\n",
    "ybar = lgr.predict(feature_test_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8253787878787879\n",
      "f1 score: 0.8253787878787879\n"
     ]
    }
   ],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)\n",
    "acc = lgr.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_tot_split)\n",
    "print(\"f1 score:\", f1_score(y_test_tot_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features with Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "k=1000\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(name_df,y)\n",
    "X_train_x2 = x2.transform(name_df)\n",
    "X_test_x2 = x2.transform(name_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721175\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_x2, y)\n",
    "acc = lgr.score(X_train_x2, y)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features with Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1000\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(name_df,y)\n",
    "X_train_mi = mi.transform(name_df)\n",
    "X_test_mi = mi.transform(name_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721175\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_mi, y)\n",
    "acc = lgr.score(X_train_mi, y)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Holdout or Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8372014925373135\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_tot_split, y_train_tot_split)\n",
    "acc = lgr.score(X_train_tot_split, y_train_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1344, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 616, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\kenne\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 770. MiB for an array with shape (32000, 3152) and data type float64\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1344, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 616, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\kenne\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 770. MiB for an array with shape (32000, 3152) and data type float64\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1344, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 616, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\kenne\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 770. MiB for an array with shape (32000, 3152) and data type float64\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1344, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 616, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\kenne\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 770. MiB for an array with shape (32000, 3152) and data type float64\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1344, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 616, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\kenne\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 770. MiB for an array with shape (32000, 3152) and data type float64\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(feature_train_tot, y)\n",
    "acc = np.mean(cross_val_score(lgr, feature_train_tot, y, cv=5))\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36984848484848487\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both Holdout and Cross-validation get similar accuracy score. However, holdout is significantly faster than cross validation and for this reason, holdout strategy will be chosen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB Accuracy: 0.36984848484848487\n",
      "5-nearest neighbour Accuracy: 0.7217424242424243\n",
      "Decision Tree Accuracy: 0.7106818181818182\n",
      "One_R Accuracy: 0.6552272727272728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "models = [GaussianNB(),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "titles = ['GNB',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree',\n",
    "          'One_R']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(X_train_tot_split,y_train_tot_split)\n",
    "    acc = model.score(X_test_tot_split,y_test_tot_split)\n",
    "    print(title, \"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.8018181818181818\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train_tot_split,y_train_tot_split)\n",
    "acc = log.score(X_test_tot_split,y_test_tot_split)\n",
    "print(\"LogisticRegression Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking, Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(), KNeighborsClassifier(), MultinomialNB()]\n",
    "titles = ['Decision Tree', 'KNeighborsClassifier()', 'Multinomial NB']\n",
    "\n",
    "\n",
    "\n",
    "meta_classifier_lr = LogisticRegression()\n",
    "stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacker Accuracy: 0.6261363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into train and test set\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y, test_size=0.33, random_state=88)\n",
    "\n",
    "stacker_lr.fit(X_train_split, y_train_split)\n",
    "acc = stacker_lr.score(X_test_split, y_test_split)\n",
    "print(\"Stacker Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "\n",
    "Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n",
    "\n",
    "Third, logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n",
    "\n",
    "Fourth, logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n",
    "\n",
    "Finally, logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7859090909090909\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced') # inverse class weighting to reduce bias\n",
    "lg2.fit(X_train_tot_split,y_train_tot_split)\n",
    "\n",
    "acc = lg2.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing features - binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_split)\n",
    "X_train_scaled = scaler.transform(X_train_split)\n",
    "X_test_scaled = scaler.transform(X_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5453787878787879\n"
     ]
    }
   ],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_scaled,y_train_tot_split)\n",
    "\n",
    "acc = lg2.score(X_test_scaled , y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does Scaling improve performance?  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_scaled,y_train_tot_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6317424242424242\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_scaled , y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling does not seem to improve the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Reducing Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 2\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# Third, logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance \n",
    "pca = PCA(n_components=0.99, whiten = True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Show results \n",
    "print(\"Original number of features:\", X_train_scaled.shape[1])\n",
    "print(\"Reduced number of features:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct PCA from fitted set\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does PCA improve performance? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_pca,y_train_tot_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6317424242424242\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_pca , y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5453787878787879\n"
     ]
    }
   ],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance + reduce correlation? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_pca,y_train_tot_split)\n",
    "\n",
    "acc = lg2.score(X_test_pca, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8018181818181818\n"
     ]
    }
   ],
   "source": [
    "lg1 = LogisticRegression()\n",
    "lg1.fit(X_train_tot_split, y_train_tot_split)\n",
    "acc = lg1.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8032575757575757\n"
     ]
    }
   ],
   "source": [
    "lg2 = LogisticRegression(C=0.1, solver='sag')\n",
    "lg2.fit(X_train_tot_split, y_train_tot_split)\n",
    "acc = lg2.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lg3 = LogisticRegression(C=0.001, solver='sag')\n",
    "lg3.fit(X_train_tot_split, y_train_tot_split)\n",
    "ybar = lg3.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sag_split_0_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg4 = LogisticRegression(C=1.0, solver='newton-cg')\n",
    "lg4.fit(feature_train_tot, y)\n",
    "ybar = lg4.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('newt_tot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lgr5 = LogisticRegression(C=1.0, solver='saga')\n",
    "lgr5.fit(X_train_tot_split, y_train_tot_split)\n",
    "ybar = lgr5.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sag_split.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
