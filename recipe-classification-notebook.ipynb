{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv_2 = \"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_test.csv\"\n",
    "train_csv_2 = \"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_train.csv\"\n",
    "\n",
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv_2)\n",
    "train_df = pd.read_csv(train_csv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['n_steps', 'n_ingredients']]\n",
    "y = train_df['duration_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X_train_name = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_name_vec.npz')\n",
    "X_test_name = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_name_vec.npz')\n",
    "\n",
    "X_train_step = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_steps_vec.npz')\n",
    "X_test_step = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_steps_vec.npz')\n",
    "\n",
    "X_train_ingr = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\train_ingr_vec.npz')\n",
    "X_test_ingr = scipy.sparse.load_npz('C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_countvec\\\\test_ingr_vec.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features with CountVec and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'name'\n",
    "k = 1000\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "name_train_mi = mi.fit_transform(X_train_name,y)\n",
    "name_test_mi = mi.transform(X_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'steps'\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "step_train_mi = mi.fit_transform(X_train_step,y)\n",
    "step_test_mi = mi.transform(X_test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Mutual Information for 'steps'\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "ingr_train_mi = mi.fit_transform(X_train_ingr,y)\n",
    "ingr_test_mi = mi.transform(X_test_ingr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Features for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = train_df.copy()\n",
    "new_train_df = new_train_df[['n_steps','n_ingredients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_matrix = name_train_mi.todense()\n",
    "name_list = name_matrix.tolist()\n",
    "name_df = pd.DataFrame(name_list)\n",
    "name_df = name_df.add_prefix('name_')\n",
    "\n",
    "step_matrix = step_train_mi.todense()\n",
    "step_list = step_matrix.tolist()\n",
    "step_df = pd.DataFrame(step_list)\n",
    "step_df = step_df.add_prefix('step_')\n",
    "\n",
    "ingr_matrix = ingr_train_mi.todense()\n",
    "ingr_list = ingr_matrix.tolist()\n",
    "ingr_df = pd.DataFrame(ingr_list)\n",
    "ingr_df = ingr_df.add_prefix('ingr_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add name, step and ingr from countvec to new_train_df, name the df features_train\n",
    "features_train = new_train_df.join(name_df)\n",
    "features_train = features_train.join(step_df)\n",
    "features_train = features_train.join(ingr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec_train = features_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = test_df.copy()\n",
    "new_test_df = new_test_df[['n_steps','n_ingredients']]\n",
    "\n",
    "name_matrix_test = name_test_mi.todense()\n",
    "name_list_test = name_matrix_test.tolist()\n",
    "name_df_test = pd.DataFrame(name_list_test)\n",
    "name_df_test = name_df_test.add_prefix('name_')\n",
    "\n",
    "step_matrix_test = step_test_mi.todense()\n",
    "step_list_test = step_matrix_test.tolist()\n",
    "step_df_test = pd.DataFrame(step_list_test)\n",
    "step_df_test = step_df_test.add_prefix('step_')\n",
    "\n",
    "ingr_matrix_test = ingr_test_mi.todense()\n",
    "ingr_list_test = ingr_matrix_test.tolist()\n",
    "ingr_df_test = pd.DataFrame(ingr_list_test)\n",
    "ingr_df_test = ingr_df_test.add_prefix('ingr_')\n",
    "\n",
    "# Add name, step and ingr from countvec to new_test_df, name the df features_test\n",
    "features_test = new_test_df.join(name_df_test)\n",
    "features_test = features_test.join(step_df_test)\n",
    "features_test = features_test.join(ingr_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec100 (does not increase accuracy, leave it out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec100 Dataset \n",
    "train_name_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_name_doc2vec100.csv\", header=None)\n",
    "test_name_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_name_doc2vec100.csv\", header=None)\n",
    "train_steps_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_steps_doc2vec100.csv\", header=None)\n",
    "test_steps_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_steps_doc2vec100.csv\", header=None)\n",
    "train_ingr_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\train_ingr_doc2vec100.csv\", header=None)\n",
    "test_ingr_doc100 = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\recipe_text_features_doc2vec100\\\\test_ingr_doc2vec100.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all Doc2Vec data to features_train, name the new df \"features_train_doc100\"\n",
    "train_name_doc100 = train_name_doc100.add_suffix('name_doc2vec')\n",
    "feature_train_doc100 = features_train.join(train_name_doc100)\n",
    "train_steps_doc100 = train_steps_doc100.add_suffix('step_doc2vec')\n",
    "feature_train_doc100 = feature_train_doc100.join(train_steps_doc100)\n",
    "train_ingr_doc100 = train_ingr_doc100.add_suffix('ingr_doc2vec')\n",
    "feature_train_doc100 = feature_train_doc100.join(train_ingr_doc100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all Doc2Vec data to features_test, name the new df \"features_test_doc100\"\n",
    "test_name_doc100 = test_name_doc100.add_suffix('name_doc2vec')\n",
    "feature_test_doc100 = features_test.join(test_name_doc100)\n",
    "test_steps_doc100 = test_steps_doc100.add_suffix('step_doc2vec')\n",
    "feature_test_doc100 = feature_test_doc100.join(test_steps_doc100)\n",
    "test_ingr_doc100 = test_ingr_doc100.add_suffix('ingr_doc2vec')\n",
    "feature_test_doc100 = feature_test_doc100.join(test_ingr_doc100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec50 Dataset \n",
    "train_name_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_name_doc2vec50.csv\", header=None)\n",
    "test_name_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_name_doc2vec50.csv\", header=None)\n",
    "train_steps_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_steps_doc2vec50.csv\", header=None)\n",
    "test_steps_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_steps_doc2vec50.csv\", header=None)\n",
    "train_ingr_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\train_ingr_doc2vec50.csv\", header=None)\n",
    "test_ingr_tot = pd.read_csv(\"C:\\\\Users\\\\kenne\\\\recipe-cooktime-predictor data\\\\COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\recipe_text_features_doc2vec50\\\\test_ingr_doc2vec50.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all Doc2Vec50 data to features_train, name the new df \"features_train_tot\"\n",
    "train_name_tot = train_name_tot.add_suffix('name_doc2vec_50')\n",
    "feature_train_tot = features_train.join(train_name_tot)\n",
    "train_steps_tot = train_steps_tot.add_suffix('step_doc2vec_50')\n",
    "feature_train_tot = feature_train_tot.join(train_steps_tot)\n",
    "train_ingr_tot = train_ingr_tot.add_suffix('ingr_doc2vec_50')\n",
    "feature_train_tot = feature_train_tot.join(train_ingr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all Doc2Vec50 data to features_test, name the new df \"features_test_tot\"\n",
    "test_name_tot = test_name_tot.add_suffix('name_doc2vec_50')\n",
    "feature_test_tot = features_test.join(test_name_tot)\n",
    "test_steps_tot = test_steps_tot.add_suffix('step_doc2vec_50')\n",
    "feature_test_tot = feature_test_tot.join(test_steps_tot)\n",
    "test_ingr_tot = test_ingr_tot.add_suffix('ingr_doc2vec_50')\n",
    "feature_test_tot = feature_test_tot.join(test_ingr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "X_train_tot_split, X_test_tot_split, y_train_tot_split, y_test_tot_split = train_test_split(feature_train_tot, y, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=0.01, solver='sag')\n",
    "lgr.fit(feature_train_tot, y)\n",
    "ybar = lgr.predict(feature_test_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8253787878787879\n",
      "f1 score: 0.8253787878787879\n"
     ]
    }
   ],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)\n",
    "acc = lgr.score(X_test_tot_split, y_test_tot_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_tot_split)\n",
    "print(\"f1 score:\", f1_score(y_test_tot_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(countvec_train, y, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features with Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "k=1000\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(name_df,y)\n",
    "X_train_x2 = x2.transform(name_df)\n",
    "X_test_x2 = x2.transform(name_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721175\n",
      "f1 score: 0.721175\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_x2, y)\n",
    "acc = lgr.score(X_train_x2, y)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_train_x2)\n",
    "print(\"f1 score:\", f1_score(y, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features with Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1000\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(name_df,y)\n",
    "X_train_mi = mi.transform(name_df)\n",
    "X_test_mi = mi.transform(name_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721175\n",
      "f1 score: 0.721175\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_mi, y)\n",
    "acc = lgr.score(X_train_mi, y)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_train_mi)\n",
    "print(\"f1 score:\", f1_score(y, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Holdout or Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7999242424242424\n",
      "f1 score: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split, y_train_split)\n",
    "acc = lgr.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7793181818181819\n",
      "f1 score: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split, y_train_split)\n",
    "acc = np.mean(cross_val_score(lgr, X_test_split, y_test_split, cv=5))\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both Holdout and Cross-validation get similar accuracy score. However, holdout is significantly faster than cross validation and for this reason, holdout strategy will be chosen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB Accuracy: 0.36984848484848487\n",
      "f1 score: 0.36984848484848487 \n",
      "\n",
      "5-nearest neighbour Accuracy: 0.7174242424242424\n",
      "f1 score: 0.7174242424242424 \n",
      "\n",
      "Decision Tree Accuracy: 0.7272727272727273\n",
      "f1 score: 0.7272727272727273 \n",
      "\n",
      "One_R Accuracy: 0.6552272727272728\n",
      "f1 score: 0.6552272727272728 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Construct different models \"\"\"\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "models = [GaussianNB(),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "titles = ['GNB',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree',\n",
    "          'One_R']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(X_train_split,y_train_split)\n",
    "    acc = model.score(X_test_split,y_test_split)\n",
    "    print(title, \"Accuracy:\",acc)\n",
    "    y_pred = model.predict(X_test_split)\n",
    "    print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.7999242424242424\n",
      "f1 score: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Compare LogisticRegression model to other models above \"\"\"\n",
    "log = LogisticRegression()\n",
    "log.fit(X_train_split,y_train_split)\n",
    "acc = log.score(X_test_split,y_test_split)\n",
    "print(\"LogisticRegression Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking, Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ensemble model \"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(), KNeighborsClassifier(), MultinomialNB()]\n",
    "titles = ['Decision Tree', 'KNeighborsClassifier()', 'Multinomial NB']\n",
    "\n",
    "\n",
    "\n",
    "meta_classifier_lr = LogisticRegression()\n",
    "stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacker Accuracy: 0.7265151515151516\n",
      "f1 score: 0.7265151515151516\n"
     ]
    }
   ],
   "source": [
    "stacker_lr.fit(X_train_split, y_train_split)\n",
    "acc = stacker_lr.score(X_test_split, y_test_split)\n",
    "print(\"Stacker Accuracy:\",acc)\n",
    "y_pred = stacker_lr.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "\n",
    "Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n",
    "\n",
    "Third, logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n",
    "\n",
    "Fourth, logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n",
    "\n",
    "Finally, logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7828030303030303\n",
      "f1 score: 0.7828030303030302\n"
     ]
    }
   ],
   "source": [
    "# Basic Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced') # inverse class weighting to reduce bias\n",
    "lg2.fit(X_train_split,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lg2.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing features - binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_split)\n",
    "X_train_scaled = scaler.transform(X_train_split)\n",
    "X_test_scaled = scaler.transform(X_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7834848484848485\n",
      "f1 score: 0.7834848484848485\n"
     ]
    }
   ],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_scaled,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_scaled, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lg2.predict(X_test_scaled)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does Scaling improve performance?  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_scaled, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7771969696969697\n",
      "f1 score: 0.7771969696969697\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_scaled, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_scaled)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling does not seem to improve the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Reducing Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 3002\n",
      "Reduced number of features: 2724\n"
     ]
    }
   ],
   "source": [
    "# Third, logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance \n",
    "pca = PCA(n_components=0.99, whiten = True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Show results \n",
    "print(\"Original number of features:\", X_train_scaled.shape[1])\n",
    "print(\"Reduced number of features:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct PCA from fitted set\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does PCA improve performance? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_pca, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7713636363636364\n",
      "f1 score: 0.7713636363636364\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_pca, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lgr.predict(X_test_pca)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7748484848484849\n",
      "f1 score: 0.7748484848484849\n"
     ]
    }
   ],
   "source": [
    "# Does PCA + fix class imbalance + use a faster solve improve performance + reduce correlation? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_pca,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_pca, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lg2.predict(X_test_pca)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7999242424242424\n",
      "f1 score: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "lg1 = LogisticRegression()\n",
    "lg1.fit(X_train_split, y_train_split)\n",
    "acc = lg1.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lg1.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8033333333333333\n",
      "f1 score: 0.8033333333333333\n"
     ]
    }
   ],
   "source": [
    "lg2 = LogisticRegression(C=0.1, solver='sag')\n",
    "lg2.fit(X_train_split, y_train_split)\n",
    "acc = lg2.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "y_pred = lg2.predict(X_test_split)\n",
    "print(\"f1 score:\", f1_score(y_test_split, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lg3 = LogisticRegression(C=0.001, solver='sag')\n",
    "lg3.fit(X_train_tot_split, y_train_tot_split)\n",
    "ybar = lg3.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sag_split_0_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lg4 = LogisticRegression(C=0.001, solver='sag')\n",
    "lg4.fit(X_train_tot_split, y_train_tot_split)\n",
    "ybar = lg4.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('tot_0_001_sag.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenne\\python\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lgr5 = LogisticRegression(C=1.0, solver='saga')\n",
    "lgr5.fit(feature_train_tot, y)\n",
    "ybar = lgr5.predict(feature_test_tot)\n",
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('newt_tot.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
