{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv = \"~/COMP30027_2021_Project2_datasets/recipe_test.csv\"\n",
    "train_csv = \"~/COMP30027_2021_Project2_datasets/recipe_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv )\n",
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load CountVec model for name \n",
    "import pickle\n",
    "name_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n",
    "name_dict = name_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CountVec model for steps\n",
    "import pickle\n",
    "step_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_steps_countvectorizer.pkl\", \"rb\"))\n",
    "step_dict = step_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CountVec model for ingredients\n",
    "import pickle\n",
    "ingr_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_ingr_countvectorizer.pkl\", \"rb\"))\n",
    "ingr_dict = ingr_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set \n",
    "X_train = train_df[['n_steps', 'n_ingredients']]\n",
    "y_train = train_df['duration_label']\n",
    "X_test = test_df[['n_steps', 'n_ingredients']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "name_train = train_df['name'].values # Create an array of name for training set\n",
    "name_test = test_df['name'].values # Create an array of name for testing set\n",
    "X_train_name = name_vectorizer.fit_transform(name_train)\n",
    "X_test_name = name_vectorizer.transform(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10892) (10000, 10892)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_name.shape, X_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Are there any documents in X_test whose values are all 0? Why might this happen?\n",
    "print(len(X_test_name.sum(axis=1)==0))\n",
    "# This is hypothetically possible - if every word in one of the test documents had never appeared in the training data.\n",
    "# For long documents, this is exceedingly unlikely due to the appearance of grammatical \"words\" such as _the_, _is_, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef\n",
      "cake\n",
      "casserole\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_name_x2 = x2.fit_transform(X_train_name,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_name_x2 = x2.transform(X_test_name)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7495 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_name_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like words that could be relevant to trying to distinguish between duration classification where salad could be made relatively fast and beef could be made relatively slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps suprisingly are words like slow, cooker, crock and crockpot which are indicative of the cooking utensils not the food itself (and perhaps not of the problem more generally). It's difficult to determine the rare/common distinction here, but it becomes a little clearer as we look further down the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "beef\n",
      "cake\n",
      "casserole\n",
      "chicken\n",
      "cooker\n",
      "crock\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_name_mi = mi.fit_transform(X_train_name,y_train)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "print(X_train_name_mi.shape, X_test_name_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see more evidence of MI choosing frequently-occuring features, such as slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier on the training dataset, and evaluate its Accuracy on the test set. Consider k-NN, and perhaps Naive Bayes or Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs likely that the dataset is still small enough that you can build a model on the entire feature set (after the CountVectorizer, but before the SelectKBest) without crashing your computer. How well do these models predict the test data, using all of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with 1000 features, or just the top 10 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different values for the cut-off for SelectKBest ‚Äî is it possible to improve upon the Accuracy observed for the models which use the entire feature set? Is this more true for some learners than others? Does your choice between œá 2 and Mutual Information make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.419725\n",
      "x2 acc 0.5769\n",
      "mi acc 0.2231\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.756375\n",
      "x2 acc 0.69515\n",
      "mi acc 0.70625\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.5185\n",
      "x2 acc 0.5185\n",
      "mi acc 0.5185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_name,y_train)\n",
    "X_train_name_x2 = x2.transform(X_train_name)\n",
    "X_test_name_x2 = x2.transform(X_test_name)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_name,y_train)\n",
    "X_train_name_mi = mi.transform(X_train_name)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_name, X_test_name), (X_train_name_x2, X_test_name_x2), (X_train_name_mi, X_test_name_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature name in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from steps column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "step_train = train_df['steps'].values # Create an array of name for training set\n",
    "step_test = test_df['steps'].values # Create an array of name for testing set\n",
    "X_train_step = step_vectorizer.fit_transform(step_train)\n",
    "X_test_step = step_vectorizer.transform(step_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 17967) (10000, 17967)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_step.shape, X_test_step.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "bake\n",
      "baking\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "hours\n",
      "minutes\n",
      "oven\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_step_x2 = x2.fit_transform(X_train_step,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_step_x2 = x2.transform(X_test_step)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(step_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best attributes for step column is similar to name column. This might due to the fact that words that describe time such as hours, minutes, slow indicate specific duration labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "30\n",
      "350\n",
      "45\n",
      "bake\n",
      "baking\n",
      "crock\n",
      "hours\n",
      "minutes\n",
      "oven\n",
      "preheat\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_step_mi = mi.fit_transform(X_train_step,y_train)\n",
    "X_test_step_mi = mi.transform(X_test_step)\n",
    "\n",
    "print(X_train_step_mi.shape, X_test_step_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(step_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could see that MI provide more words that describe time such as hours, minutes, 30, 350 and 45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.40335\n",
      "x2 acc 0.650025\n",
      "mi acc 0.5815\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.751825\n",
      "x2 acc 0.711875\n",
      "mi acc 0.716325\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.6481\n",
      "x2 acc 0.6481\n",
      "mi acc 0.6481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_step,y_train)\n",
    "X_train_step_x2 = x2.transform(X_train_step)\n",
    "X_test_step_x2 = x2.transform(X_test_step)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_step,y_train)\n",
    "X_train_step_mi = mi.transform(X_train_step)\n",
    "X_test_step_mi = mi.transform(X_test_step)\n",
    "\n",
    "# Chi-square -> (X_train_step_x2, X_test_step_x2)\n",
    "# Mutual Information -> (X_train_step_mi, X_test_step_mi) \n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_step, X_test_step), (X_train_step_x2, X_test_step_x2), (X_train_step_mi, X_test_step_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature step in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ingr_train = train_df['ingredients'].values # Create an array of name for training set\n",
    "ingr_test = test_df['ingredients'].values # Create an array of name for testing set\n",
    "X_train_ingr = ingr_vectorizer.fit_transform(ingr_train)\n",
    "X_test_ingr = ingr_vectorizer.transform(ingr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2906) (10000, 2906)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ingr.shape, X_test_ingr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baking\n",
      "beef\n",
      "butter\n",
      "chicken\n",
      "eggs\n",
      "flour\n",
      "potatoes\n",
      "roast\n",
      "soup\n",
      "stew\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_ingr_x2 = x2.fit_transform(X_train_ingr,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_ingr_x2 = x2.transform(X_test_ingr)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(ingr_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ingredients colum, we could observe that both the cooking materials and cooking methods are selected as among the best attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "baking\n",
      "beef\n",
      "butter\n",
      "eggs\n",
      "flour\n",
      "onion\n",
      "potatoes\n",
      "roast\n",
      "salt\n",
      "sugar\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_ingr_mi = mi.fit_transform(X_train_ingr,y_train)\n",
    "X_test_ingr_mi = mi.transform(X_test_ingr)\n",
    "\n",
    "print(X_train_ingr_mi.shape, X_test_ingr_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(ingr_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to chi-square. Both cooking methods and cooking materials are among the best attribites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.189275\n",
      "x2 acc 0.51075\n",
      "mi acc 0.23255\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.638575\n",
      "x2 acc 0.627375\n",
      "mi acc 0.62705\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.537825\n",
      "x2 acc 0.537825\n",
      "mi acc 0.537825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_ingr,y_train)\n",
    "X_train_ingr_x2 = x2.transform(X_train_ingr)\n",
    "X_test_ingr_x2 = x2.transform(X_test_ingr)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_ingr,y_train)\n",
    "X_train_ingr_mi = mi.transform(X_train_ingr)\n",
    "X_test_ingr_mi = mi.transform(X_test_ingr)\n",
    "\n",
    "# Chi-square -> (X_train_ingr_x2, X_test_ingr_x2)\n",
    "# Mutual Information -> (X_train_ingr_mi, X_test_ingr_mi)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_ingr, X_test_ingr), (X_train_ingr_x2, X_test_ingr_x2) , (X_train_ingr_mi, X_test_ingr_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature ingredients in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Training Features for CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sparse matrix for name\n",
    "mi_matrix = X_train_name_mi.todense()\n",
    "# Convert matrix to list\n",
    "mi_list = mi_matrix.tolist()\n",
    "# Convert list to dataframe\n",
    "mi_df = pd.DataFrame(mi_list)\n",
    "mi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of name in training set\n",
    "# mi_df.to_csv('mi_train_name_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1002)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the name features to the training set\n",
    "join_train_df = X_train.join(mi_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_matrix = X_train_step_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_step_df = pd.DataFrame(mi_list)\n",
    "\n",
    "mi_step_df = mi_step_df.add_suffix('step')\n",
    "mi_step_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in training set\n",
    "# mi_step_df.to_csv('mi_train_step_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2002)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_train_df = join_train_df.join(mi_step_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_matrix = X_train_ingr_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_ingr_df = pd.DataFrame(mi_list)\n",
    "\n",
    "mi_ingr_df = mi_ingr_df.add_suffix('ingr')\n",
    "\n",
    "mi_ingr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in training set\n",
    "# mi_ingr_df.to_csv('mi_train_ingr_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3002)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_train_df = join_train_df.join(mi_ingr_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Bag of Words in training set\n",
    "# join_train_df.to_csv('join_train_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Test Features For CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_matrix = X_test_name_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_name_df = pd.DataFrame(mi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of name in testing set\n",
    "# mi_name_df.to_csv('mi_test_name_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1002)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = X_test.join(mi_name_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI for step\n",
    "mi_matrix = X_test_step_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_step_df = pd.DataFrame(mi_list)\n",
    "mi_step_df = mi_step_df.add_suffix('step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in testing set\n",
    "# mi_step_df.to_csv('mi_test_step_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2002)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = join_test_df.join(mi_step_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI for ingr\n",
    "mi_matrix = X_test_ingr_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_ingr_df = pd.DataFrame(mi_list)\n",
    "mi_ingr_df = mi_ingr_df.add_suffix('ingr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of ingr in testing set\n",
    "# mi_ingr_df.to_csv('mi_test_ingr_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3002)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = join_test_df.join(mi_ingr_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Bag of Words in testing set\n",
    "# join_test_df.to_csv('join_test_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-46-d7201a8039fa>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-d7201a8039fa>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    ''''\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          #KNeighborsClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          #'KNN',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(join_train_df,y_train)\n",
    "    acc = model.score(join_train_df, y_train)\n",
    "    print(title, \"Accuracy:\",acc)\n",
    "''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Holdout or Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold out Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(join_train_df, y_train, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split,y_train_split)\n",
    "acc = lgr.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(join_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79975\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(cross_val_score(lgr, join_train_df, y_train, cv=5))\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Holdout and Cross-validation get similar accuracy score. However, holdout is significantly faster than cross validation and for this reason, holdout strategy will be chosen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(join_train_df, y_train, test_size=0.2,stratify=y_train,random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.364625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_split, y_train_split)\n",
    "acc = gnb.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.706125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_split, y_train_split)\n",
    "acc = mnb.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.641125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "one_r =DecisionTreeClassifier(max_depth=1)\n",
    "one_r.fit(X_train_split, y_train_split)\n",
    "acc = one_r.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.732375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_split, y_train_split)\n",
    "acc = dt.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Dataset \n",
    "train_name_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "test_name_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/test_name_doc2vec100.csv\", header=None)\n",
    "train_steps_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "test_steps_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/test_steps_doc2vec100.csv\", header=None)\n",
    "train_ingr_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "test_ingr_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/test_ingr_doc2vec100.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100) (40000, 100) (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_name_doc2vec100.shape, train_steps_doc2vec100.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Dataset \n",
    "train_name_doc2vec50 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec50/train_name_doc2vec50.csv\", header=None)\n",
    "test_name_doc2vec50 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec50/test_name_doc2vec50.csv\", header=None)\n",
    "train_steps_doc2vec50 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec50/train_steps_doc2vec50.csv\", header=None)\n",
    "test_steps_doc2vec50 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec50/test_steps_doc2vec50.csv\", header=None)\n",
    "train_ingr_doc2vec50 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec50/train_ingr_doc2vec50.csv\", header=None)\n",
    "test_ingr_doc2vec50 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec50/test_ingr_doc2vec50.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 50) (40000, 50) (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_name_doc2vec50.shape, train_steps_doc2vec50.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 300)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining Training Features for Doc2Vec100\n",
    "train_name_doc2vec100 = train_name_doc2vec100.add_suffix('name_doc2vec100')\n",
    "join_train_doc2vec100 = train_name_doc2vec100\n",
    "train_steps_doc2vec100 = train_steps_doc2vec100.add_suffix('step_doc2vec100')\n",
    "join_train_doc2vec100 = join_train_doc2vec100.join(train_steps_doc2vec100)\n",
    "train_ingr_doc2vec100 = train_ingr_doc2vec100.add_suffix('ingr_doc2vec100')\n",
    "join_train_doc2vec100 = join_train_doc2vec100.join(train_ingr_doc2vec100)\n",
    "join_train_doc2vec100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining Testing Features for Doc2Vec100\n",
    "test_name_doc2vec100 = test_name_doc2vec100.add_suffix('name_doc2vec100')\n",
    "join_test_doc2vec100 = test_name_doc2vec100\n",
    "test_steps_doc2vec100 = test_steps_doc2vec100.add_suffix('step_doc2vec100')\n",
    "join_test_doc2vec100 = join_test_doc2vec100.join(test_steps_doc2vec100)\n",
    "test_ingr_doc2vec100 = test_ingr_doc2vec100.add_suffix('ingr_doc2vec100')\n",
    "join_test_doc2vec100 = join_test_doc2vec100.join(test_ingr_doc2vec100)\n",
    "join_test_doc2vec100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 150)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining Training Features for Doc2Vec50\n",
    "train_name_doc2vec50 = train_name_doc2vec50.add_suffix('name_doc2vec50')\n",
    "join_train_doc2vec50 = train_name_doc2vec50\n",
    "train_steps_doc2vec50 = train_steps_doc2vec50.add_suffix('step_doc2vec50')\n",
    "join_train_doc2vec50 = join_train_doc2vec50.join(train_steps_doc2vec50)\n",
    "train_ingr_doc2vec50 = train_ingr_doc2vec50.add_suffix('ingr_doc2vec50')\n",
    "join_train_doc2vec50 = join_train_doc2vec50.join(train_ingr_doc2vec50)\n",
    "join_train_doc2vec50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 150)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining Testing Features for Doc2Vec100\n",
    "test_name_doc2vec50 = test_name_doc2vec50.add_suffix('name_doc2vec50')\n",
    "join_test_doc2vec50 = test_name_doc2vec50\n",
    "test_steps_doc2vec50 = test_steps_doc2vec50.add_suffix('step_doc2vec50')\n",
    "join_test_doc2vec50 = join_test_doc2vec50.join(test_steps_doc2vec50)\n",
    "test_ingr_doc2vec50 = test_ingr_doc2vec50.add_suffix('ingr_doc2vec50')\n",
    "join_test_doc2vec50 = join_test_doc2vec50.join(test_ingr_doc2vec50)\n",
    "join_test_doc2vec50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3452)\n"
     ]
    }
   ],
   "source": [
    "# Join all training features\n",
    "join_train_all = join_train_df.join(join_train_doc2vec100)\n",
    "join_train_all = join_train_all.join(join_train_doc2vec50)\n",
    "print(join_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3452)\n"
     ]
    }
   ],
   "source": [
    "# Join all testing features\n",
    "join_test_all = join_test_df.join(join_test_doc2vec100)\n",
    "join_test_all = join_test_all.join(join_test_doc2vec50)\n",
    "print(join_test_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(join_train_all, y_train, test_size=0.2,stratify=y_train,random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split,y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.786875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "acc = lgr.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)\n",
    "#print(\"f1 score:\", f1_score(y_train_split, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression without validation set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(join_train_all,y_train)\n",
    "\n",
    "acc = lgr.score(join_train_all, y_train)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.506150\n",
       "1    0.442625\n",
       "3    0.051225\n",
       "Name: duration_label, dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check distribution\n",
    "y_train_imbalance = y_train.astype(int)\n",
    "y_train_imbalance.value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class weight for 1 is 0.44, Class weight for 2 is 0.506 and Class Weight for 3 is 0.05. And there is a significant class imbalnce in Duration Label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced') # inverse class weighting to reduce bias\n",
    "lg2.fit(X_train_split,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8192\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression without validation set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced')\n",
    "lg2.fit(join_train_all, y_train)\n",
    "\n",
    "#acc = lg2.score(join_train_all, y_train)\n",
    "#print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing features - binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_split)\n",
    "X_train_scaled = scaler.transform(X_train_split)\n",
    "X_test_scaled = scaler.transform(X_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770625\n"
     ]
    }
   ],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_scaled,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_scaled , y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does Scaling improve performance?  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_scaled,y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.771125\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_scaled , y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling does not seem to improve the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Reduce Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 3452\n",
      "Reduced number of features: 2984\n"
     ]
    }
   ],
   "source": [
    "# Third, logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance \n",
    "pca = PCA(n_components=0.99, whiten = True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Show results \n",
    "print(\"Original number of features:\", X_train_scaled.shape[1])\n",
    "print(\"Reduced number of features:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct PCA from fitted set\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does PCA improve performance? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_pca,y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.769875\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_pca , y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Does Scaling + fix class imbalance + use a faster solve improve performance + reduce correlation? \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg2 = LogisticRegression(class_weight='balanced', solver = \"sag\") # inverse class weighting to reduce bias, solve faster\n",
    "lg2.fit(X_train_pca,y_train_split)\n",
    "\n",
    "acc = lg2.score(X_test_pca, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [#DummyClassifier(strategy='most_frequent'),\n",
    "                LogisticRegression(),\n",
    "                #KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                #MultinomialNB(),\n",
    "                DecisionTreeClassifier()]\n",
    "titles = [#'Zero_R',\n",
    "          'Logistic Regression',\n",
    "          #'KNN',\n",
    "          'Gaussian NB',  \n",
    "          #'Multinomial NB',\n",
    "          'Decision Tree']\n",
    "\n",
    "\n",
    "\n",
    "meta_classifier_lr = LogisticRegression()\n",
    "stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacker Accuracy (Logistic Regression): 0.703125\n"
     ]
    }
   ],
   "source": [
    "# Run with validation set\n",
    "stacker_lr.fit(X_train_split,y_train_split)\n",
    "print('\\nStacker Accuracy (Logistic Regression):', stacker_lr.score(X_test_split, y_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacker Accuracy (Logistic Regression): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Run without validation set\n",
    "stacker_lr.fit(join_train_all, y_train)\n",
    "print('\\nStacker Accuracy (Logistic Regression):', stacker_lr.score(X_test_split, y_test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar = lgr.predict(join_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Neural Network\\nfrom keras import models\\nfrom keras import layers\\nfrom keras.utils.np_utils import to_categorical\\n\\nfrom sklearn import preprocessing\\n\\nX_train_split_neural = X_train_split\\nX_test_split_neural = X_test_split\\n\\n#Create Scalar\\nscaler = preprocessing.StandardScaler()\\n\\n# Transform the feature\\nX_train_split_neural = scaler.fit_transform(X_train_split_neural)\\nX_test_split_neural = scaler.fit_transform(X_test_split_neural)\\n\\nprint(X_train_split_neural[:,0].mean(),X_train_split_neural[:,0].std())\\n\\ny_train_split_neural = y_train_split - 1\\ny_test_split_neural = y_test_split - 1\\n\\n# One-hot encode target vector to create a target matrix\\ny_train_split_cat = to_categorical(y_train_split_neural)\\ny_test_split_cat = to_categorical(y_test_split_neural)\\n\\nprint(y_train_split_cat.shape)\\n\\n# Set the number of featuers we want\\nnumber_of_features = 3002\\n\\n# Start neural network\\nnetwork = models.Sequential()\\n\\n# Add an embedding layer\\nnetwork.add(layers.Dense(units=100, activation=\"relu\", input_shape=(number_of_features,)))\\n\\n# Add a long short-term memory layer with 128 units\\nnetwork.add(layers.Dense(units=100, activation = \"relu\"))\\n\\n#Add fully connected layer with a softmax activation function\\nnetwork.add(layers.Dense(units=3, activation=\"softmax\"))\\n\\n#Compile neural network\\nnetwork.compile(loss=\"categorical_crossentropy\", #Cross-entropy\\n               optimizer = \"Adam\", # Adam Optimization\\n               metrics=[\"accuracy\"]) # Accuracy performance metric\\n\\n# Train neural network\\nhistory = network.fit(X_train_split_neural, #Features\\n                     y_train_split_cat, #Target\\n                     epochs=10, #one epoch\\n                     verbose=1, #No output\\n                     batch_size=100, #Number of observations per batch\\n                     validation_data=(X_test_split_neural, y_test_split_cat)) # test data\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Neural Network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train_split_neural = X_train_split\n",
    "X_test_split_neural = X_test_split\n",
    "\n",
    "#Create Scalar\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Transform the feature\n",
    "X_train_split_neural = scaler.fit_transform(X_train_split_neural)\n",
    "X_test_split_neural = scaler.fit_transform(X_test_split_neural)\n",
    "\n",
    "print(X_train_split_neural[:,0].mean(),X_train_split_neural[:,0].std())\n",
    "\n",
    "y_train_split_neural = y_train_split - 1\n",
    "y_test_split_neural = y_test_split - 1\n",
    "\n",
    "# One-hot encode target vector to create a target matrix\n",
    "y_train_split_cat = to_categorical(y_train_split_neural)\n",
    "y_test_split_cat = to_categorical(y_test_split_neural)\n",
    "\n",
    "print(y_train_split_cat.shape)\n",
    "\n",
    "# Set the number of featuers we want\n",
    "number_of_features = 3002\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "network.add(layers.Dense(units=100, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "\n",
    "# Add a long short-term memory layer with 128 units\n",
    "network.add(layers.Dense(units=100, activation = \"relu\"))\n",
    "\n",
    "#Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "#Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", #Cross-entropy\n",
    "               optimizer = \"Adam\", # Adam Optimization\n",
    "               metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(X_train_split_neural, #Features\n",
    "                     y_train_split_cat, #Target\n",
    "                     epochs=10, #one epoch\n",
    "                     verbose=1, #No output\n",
    "                     batch_size=100, #Number of observations per batch\n",
    "                     validation_data=(X_test_split_neural, y_test_split_cat)) # test data\n",
    "                     \n",
    "# Change the Test Dataset for prediction\n",
    "join_test_neural = join_test_df\n",
    "# Transform the feature \n",
    "join_test_neural = scaler.fit_transform(join_test_neural)\n",
    "print(join_test_neural[:,0].mean(),join_test_neural[:,0].std())\n",
    "\n",
    "# Prediction \n",
    "ybar = network.predict(join_test_neural) \n",
    "ybar\n",
    "\n",
    "# Post process prediction \n",
    "y_bar_list = []\n",
    "for row in ybar:\n",
    "    max_value = max(row)\n",
    "    for column in range(len(row)):\n",
    "        if row[column] == max_value:\n",
    "            max_index = column + 1\n",
    "            y_bar_list.append(max_index)\n",
    "            \n",
    "ybar = lgr.predict(join_test_all)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
