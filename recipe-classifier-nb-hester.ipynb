{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv = \"~/COMP30027_2021_Project2_datasets/recipe_test.csv\"\n",
    "train_csv = \"~/COMP30027_2021_Project2_datasets/recipe_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv )\n",
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load CountVec model for name \n",
    "import pickle\n",
    "name_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n",
    "name_dict = name_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CountVec model for steps\n",
    "import pickle\n",
    "step_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_steps_countvectorizer.pkl\", \"rb\"))\n",
    "step_dict = step_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CountVec model for ingredients\n",
    "import pickle\n",
    "ingr_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_ingr_countvectorizer.pkl\", \"rb\"))\n",
    "ingr_dict = ingr_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set \n",
    "X_train = train_df[['n_steps', 'n_ingredients']]\n",
    "y_train = train_df['duration_label']\n",
    "X_test = test_df[['n_steps', 'n_ingredients']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "name_train = train_df['name'].values # Create an array of name for training set\n",
    "name_test = test_df['name'].values # Create an array of name for testing set\n",
    "X_train_name = name_vectorizer.fit_transform(name_train)\n",
    "X_test_name = name_vectorizer.transform(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10892) (10000, 10892)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_name.shape, X_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Are there any documents in X_test whose values are all 0? Why might this happen?\n",
    "print(len(X_test_name.sum(axis=1)==0))\n",
    "# This is hypothetically possible - if every word in one of the test documents had never appeared in the training data.\n",
    "# For long documents, this is exceedingly unlikely due to the appearance of grammatical \"words\" such as _the_, _is_, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef\n",
      "cake\n",
      "casserole\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_name_x2 = x2.fit_transform(X_train_name,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_name_x2 = x2.transform(X_test_name)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7495 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_name_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like words that could be relevant to trying to distinguish between duration classification where salad could be made relatively fast and beef could be made relatively slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps suprisingly are words like slow, cooker, crock and crockpot which are indicative of the cooking utensils not the food itself (and perhaps not of the problem more generally). It's difficult to determine the rare/common distinction here, but it becomes a little clearer as we look further down the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "beef\n",
      "cake\n",
      "casserole\n",
      "chicken\n",
      "cooker\n",
      "crock\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_name_mi = mi.fit_transform(X_train_name,y_train)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "print(X_train_name_mi.shape, X_test_name_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see more evidence of MI choosing frequently-occuring features, such as slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier on the training dataset, and evaluate its Accuracy on the test set. Consider k-NN, and perhaps Naive Bayes or Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs likely that the dataset is still small enough that you can build a model on the entire feature set (after the CountVectorizer, but before the SelectKBest) without crashing your computer. How well do these models predict the test data, using all of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with 1000 features, or just the top 10 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different values for the cut-off for SelectKBest ‚Äî is it possible to improve upon the Accuracy observed for the models which use the entire feature set? Is this more true for some learners than others? Does your choice between œá 2 and Mutual Information make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.419725\n",
      "x2 acc 0.5769\n",
      "mi acc 0.2231\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.756375\n",
      "x2 acc 0.69515\n",
      "mi acc 0.70625\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.5185\n",
      "x2 acc 0.5185\n",
      "mi acc 0.5185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_name,y_train)\n",
    "X_train_name_x2 = x2.transform(X_train_name)\n",
    "X_test_name_x2 = x2.transform(X_test_name)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_name,y_train)\n",
    "X_train_name_mi = mi.transform(X_train_name)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_name, X_test_name), (X_train_name_x2, X_test_name_x2), (X_train_name_mi, X_test_name_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature name in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from steps column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "step_train = train_df['steps'].values # Create an array of name for training set\n",
    "step_test = test_df['steps'].values # Create an array of name for testing set\n",
    "X_train_step = step_vectorizer.fit_transform(step_train)\n",
    "X_test_step = step_vectorizer.transform(step_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 17967) (10000, 17967)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_step.shape, X_test_step.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "bake\n",
      "baking\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "hours\n",
      "minutes\n",
      "oven\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_step_x2 = x2.fit_transform(X_train_step,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_step_x2 = x2.transform(X_test_step)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(step_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best attributes for step column is similar to name column. This might due to the fact that words that describe time such as hours, minutes, slow indicate specific duration labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "30\n",
      "350\n",
      "45\n",
      "bake\n",
      "baking\n",
      "crock\n",
      "hours\n",
      "minutes\n",
      "oven\n",
      "preheat\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_step_mi = mi.fit_transform(X_train_step,y_train)\n",
    "X_test_step_mi = mi.transform(X_test_step)\n",
    "\n",
    "print(X_train_step_mi.shape, X_test_step_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(step_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could see that MI provide more words that describe time such as hours, minutes, 30, 350 and 45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.40335\n",
      "x2 acc 0.650025\n",
      "mi acc 0.5815\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.751825\n",
      "x2 acc 0.711875\n",
      "mi acc 0.716325\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.6481\n",
      "x2 acc 0.6481\n",
      "mi acc 0.6481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_step,y_train)\n",
    "X_train_step_x2 = x2.transform(X_train_step)\n",
    "X_test_step_x2 = x2.transform(X_test_step)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_step,y_train)\n",
    "X_train_step_mi = mi.transform(X_train_step)\n",
    "X_test_step_mi = mi.transform(X_test_step)\n",
    "\n",
    "# Chi-square -> (X_train_step_x2, X_test_step_x2)\n",
    "# Mutual Information -> (X_train_step_mi, X_test_step_mi) \n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_step, X_test_step), (X_train_step_x2, X_test_step_x2), (X_train_step_mi, X_test_step_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature step in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ingr_train = train_df['ingredients'].values # Create an array of name for training set\n",
    "ingr_test = test_df['ingredients'].values # Create an array of name for testing set\n",
    "X_train_ingr = ingr_vectorizer.fit_transform(ingr_train)\n",
    "X_test_ingr = ingr_vectorizer.transform(ingr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2906) (10000, 2906)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ingr.shape, X_test_ingr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baking\n",
      "beef\n",
      "butter\n",
      "chicken\n",
      "eggs\n",
      "flour\n",
      "potatoes\n",
      "roast\n",
      "soup\n",
      "stew\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_ingr_x2 = x2.fit_transform(X_train_ingr,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_ingr_x2 = x2.transform(X_test_ingr)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(ingr_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ingredients colum, we could observe that both the cooking materials and cooking methods are selected as among the best attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "baking\n",
      "beef\n",
      "butter\n",
      "eggs\n",
      "flour\n",
      "onion\n",
      "potatoes\n",
      "roast\n",
      "salt\n",
      "sugar\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_ingr_mi = mi.fit_transform(X_train_ingr,y_train)\n",
    "X_test_ingr_mi = mi.transform(X_test_ingr)\n",
    "\n",
    "print(X_train_ingr_mi.shape, X_test_ingr_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(ingr_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to chi-square. Both cooking methods and cooking materials are among the best attribites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.189275\n",
      "x2 acc 0.51075\n",
      "mi acc 0.23255\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.638575\n",
      "x2 acc 0.627375\n",
      "mi acc 0.62705\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.537825\n",
      "x2 acc 0.537825\n",
      "mi acc 0.537825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_ingr,y_train)\n",
    "X_train_ingr_x2 = x2.transform(X_train_ingr)\n",
    "X_test_ingr_x2 = x2.transform(X_test_ingr)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_ingr,y_train)\n",
    "X_train_ingr_mi = mi.transform(X_train_ingr)\n",
    "X_test_ingr_mi = mi.transform(X_test_ingr)\n",
    "\n",
    "# Chi-square -> (X_train_ingr_x2, X_test_ingr_x2)\n",
    "# Mutual Information -> (X_train_ingr_mi, X_test_ingr_mi)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_ingr, X_test_ingr), (X_train_ingr_x2, X_test_ingr_x2) , (X_train_ingr_mi, X_test_ingr_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature ingredients in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use columns with string\n",
    "new_train_df = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sparse matrix for name\n",
    "mi_matrix = X_train_name_mi.todense()\n",
    "# Convert matrix to list\n",
    "mi_list = mi_matrix.tolist()\n",
    "# Convert list to dataframe\n",
    "mi_df = pd.DataFrame(mi_list)\n",
    "mi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of name in training set\n",
    "# mi_df.to_csv('mi_train_name_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1002)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the name features to the training set\n",
    "join_train_df = X_train.join(mi_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_matrix = X_train_step_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_step_df = pd.DataFrame(mi_list)\n",
    "\n",
    "mi_step_df = mi_step_df.add_suffix('step')\n",
    "mi_step_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in training set\n",
    "# mi_step_df.to_csv('mi_train_step_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2002)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_train_df = join_train_df.join(mi_step_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_matrix = X_train_ingr_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_ingr_df = pd.DataFrame(mi_list)\n",
    "\n",
    "mi_ingr_df = mi_ingr_df.add_suffix('ingr')\n",
    "\n",
    "mi_ingr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in training set\n",
    "# mi_ingr_df.to_csv('mi_train_ingr_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3002)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_train_df = join_train_df.join(mi_ingr_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6ca87d929f00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_train_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 208\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.values_from_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4686\u001b[0m                [nan,  3.]])\n\u001b[1;32m   4687\u001b[0m         \"\"\"\n\u001b[0;32m-> 4688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dtype_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4630\u001b[0m         \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRetrieving\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4631\u001b[0m         \"\"\"\n\u001b[0;32m-> 4632\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4437\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4439\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   4426\u001b[0m         \"\"\"\n\u001b[1;32m   4427\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4428\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4436\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4096\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4097\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4098\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4099\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   5067\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5068\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[0;32m-> 5069\u001b[0;31m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[1;32m   5070\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5071\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   5090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5091\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5092\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5093\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          #KNeighborsClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          #'KNN',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(join_train_df,y_train)\n",
    "    acc = model.score(join_train_df, y_train)\n",
    "    print(title, \"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# acc = np.mean(cross_val_score(model, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_matrix = X_test_name_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_name_df = pd.DataFrame(mi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of name in testing set\n",
    "# mi_name_df.to_csv('mi_test_name_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1002)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = X_test.join(mi_name_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI for step\n",
    "mi_matrix = X_test_step_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_step_df = pd.DataFrame(mi_list)\n",
    "mi_step_df = mi_step_df.add_suffix('step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in testing set\n",
    "# mi_step_df.to_csv('mi_test_step_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2002)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = join_test_df.join(mi_step_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI for ingr\n",
    "mi_matrix = X_test_ingr_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_ingr_df = pd.DataFrame(mi_list)\n",
    "mi_ingr_df = mi_ingr_df.add_suffix('ingr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of ingr in testing set\n",
    "# mi_ingr_df.to_csv('mi_test_ingr_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3002)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = join_test_df.join(mi_ingr_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression()\n",
    "log.fit(join_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.score(join_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar = log.predict(join_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
