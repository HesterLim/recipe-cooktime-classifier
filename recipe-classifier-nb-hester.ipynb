{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv = \"~/COMP30027_2021_Project2_datasets/recipe_test.csv\"\n",
    "train_csv = \"~/COMP30027_2021_Project2_datasets/recipe_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv )\n",
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load CountVec model\n",
    "import pickle\n",
    "name_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n",
    "name_dict = name_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set \n",
    "X_train = train_df[['n_steps', 'n_ingredients']]\n",
    "y_train = train_df['duration_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "name_train = train_df['name'].values # Create an array of name for training set\n",
    "name_test = test_df['name'].values # Create an array of name for testing set\n",
    "X_train_name = name_vectorizer.fit_transform(name_train)\n",
    "X_test_name = name_vectorizer.transform(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10892) (10000, 10892)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_name.shape, X_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Are there any documents in X_test whose values are all 0? Why might this happen?\n",
    "print(len(X_test_name.sum(axis=1)==0))\n",
    "# This is hypothetically possible - if every word in one of the test documents had never appeared in the training data.\n",
    "# For long documents, this is exceedingly unlikely due to the appearance of grammatical \"words\" such as _the_, _is_, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef\n",
      "cake\n",
      "casserole\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_x2 = x2.fit_transform(X_train_name,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_x2 = x2.transform(X_test_name)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 81901 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like words that could be relevant to trying to distinguish between duration classification where salad could be made relatively fast and beef could be made relatively slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps suprisingly are words like slow, cooker, crock and crockpot which are indicative of the cooking utensils not the food itself (and perhaps not of the problem more generally). It's difficult to determine the rare/common distinction here, but it becomes a little clearer as we look further down the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "beef\n",
      "cake\n",
      "casserole\n",
      "chicken\n",
      "cooker\n",
      "crock\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_name_mi = mi.fit_transform(X_train_name,y_train)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "print(X_train_name_mi.shape, X_test_name_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see more evidence of MI choosing frequently-occuring features, such as slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier on the training dataset, and evaluate its Accuracy on the test set. Consider k-NN, and perhaps Naive Bayes or Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs likely that the dataset is still small enough that you can build a model on the entire feature set (after the CountVectorizer, but before the SelectKBest) without crashing your computer. How well do these models predict the test data, using all of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with 1000 features, or just the top 10 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different values for the cut-off for SelectKBest ‚Äî is it possible to improve upon the Accuracy observed for the models which use the entire feature set? Is this more true for some learners than others? Does your choice between œá 2 and Mutual Information make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.419725\n",
      "x2 acc 0.5769\n",
      "mi acc 0.2231\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.756375\n",
      "x2 acc 0.69515\n",
      "mi acc 0.70625\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.5185\n",
      "x2 acc 0.5185\n",
      "mi acc 0.5185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_name,y_train)\n",
    "X_train_x2 = x2.transform(X_train_name)\n",
    "X_test_x2 = x2.transform(X_test_name)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_name,y_train)\n",
    "X_train_mi = mi.transform(X_train_name)\n",
    "X_test_mi = mi.transform(X_test_name)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_name, X_test_name), (X_train_x2, X_test_x2), (X_train_mi, X_test_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature name in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
