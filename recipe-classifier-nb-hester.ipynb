{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "test_csv = \"~/COMP30027_2021_Project2_datasets/recipe_test.csv\"\n",
    "train_csv = \"~/COMP30027_2021_Project2_datasets/recipe_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "test_df = pd.read_csv(test_csv )\n",
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load CountVec model for name \n",
    "import pickle\n",
    "name_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n",
    "name_dict = name_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CountVec model for steps\n",
    "import pickle\n",
    "step_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_steps_countvectorizer.pkl\", \"rb\"))\n",
    "step_dict = step_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CountVec model for ingredients\n",
    "import pickle\n",
    "ingr_vectorizer = pickle.load(open(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_ingr_countvectorizer.pkl\", \"rb\"))\n",
    "ingr_dict = ingr_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set \n",
    "X_train = train_df[['n_steps', 'n_ingredients']]\n",
    "y_train = train_df['duration_label']\n",
    "X_test = test_df[['n_steps', 'n_ingredients']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "name_train = train_df['name'].values # Create an array of name for training set\n",
    "name_test = test_df['name'].values # Create an array of name for testing set\n",
    "X_train_name = name_vectorizer.fit_transform(name_train)\n",
    "X_test_name = name_vectorizer.transform(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10892) (10000, 10892)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_name.shape, X_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Are there any documents in X_test whose values are all 0? Why might this happen?\n",
    "print(len(X_test_name.sum(axis=1)==0))\n",
    "# This is hypothetically possible - if every word in one of the test documents had never appeared in the training data.\n",
    "# For long documents, this is exceedingly unlikely due to the appearance of grammatical \"words\" such as _the_, _is_, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef\n",
      "cake\n",
      "casserole\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_name_x2 = x2.fit_transform(X_train_name,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_name_x2 = x2.transform(X_test_name)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7495 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_name_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like words that could be relevant to trying to distinguish between duration classification where salad could be made relatively fast and beef could be made relatively slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps suprisingly are words like slow, cooker, crock and crockpot which are indicative of the cooking utensils not the food itself (and perhaps not of the problem more generally). It's difficult to determine the rare/common distinction here, but it becomes a little clearer as we look further down the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "beef\n",
      "cake\n",
      "casserole\n",
      "chicken\n",
      "cooker\n",
      "crock\n",
      "pot\n",
      "roast\n",
      "salad\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_name_mi = mi.fit_transform(X_train_name,y_train)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "print(X_train_name_mi.shape, X_test_name_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(name_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see more evidence of MI choosing frequently-occuring features, such as slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier on the training dataset, and evaluate its Accuracy on the test set. Consider k-NN, and perhaps Naive Bayes or Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs likely that the dataset is still small enough that you can build a model on the entire feature set (after the CountVectorizer, but before the SelectKBest) without crashing your computer. How well do these models predict the test data, using all of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with 1000 features, or just the top 10 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different values for the cut-off for SelectKBest ‚Äî is it possible to improve upon the Accuracy observed for the models which use the entire feature set? Is this more true for some learners than others? Does your choice between œá 2 and Mutual Information make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.419725\n",
      "x2 acc 0.5769\n",
      "mi acc 0.2231\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.756375\n",
      "x2 acc 0.69515\n",
      "mi acc 0.70625\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.5185\n",
      "x2 acc 0.5185\n",
      "mi acc 0.5185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_name,y_train)\n",
    "X_train_name_x2 = x2.transform(X_train_name)\n",
    "X_test_name_x2 = x2.transform(X_test_name)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_name,y_train)\n",
    "X_train_name_mi = mi.transform(X_train_name)\n",
    "X_test_name_mi = mi.transform(X_test_name)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_name, X_test_name), (X_train_name_x2, X_test_name_x2), (X_train_name_mi, X_test_name_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature name in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from steps column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "step_train = train_df['steps'].values # Create an array of name for training set\n",
    "step_test = test_df['steps'].values # Create an array of name for testing set\n",
    "X_train_step = step_vectorizer.fit_transform(step_train)\n",
    "X_test_step = step_vectorizer.transform(step_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 17967) (10000, 17967)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_step.shape, X_test_step.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "bake\n",
      "baking\n",
      "cooker\n",
      "crock\n",
      "crockpot\n",
      "hours\n",
      "minutes\n",
      "oven\n",
      "slow\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_step_x2 = x2.fit_transform(X_train_step,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_step_x2 = x2.transform(X_test_step)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(step_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best attributes for step column is similar to name column. This might due to the fact that words that describe time such as hours, minutes, slow indicate specific duration labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "30\n",
      "350\n",
      "45\n",
      "bake\n",
      "baking\n",
      "crock\n",
      "hours\n",
      "minutes\n",
      "oven\n",
      "preheat\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_step_mi = mi.fit_transform(X_train_step,y_train)\n",
    "X_test_step_mi = mi.transform(X_test_step)\n",
    "\n",
    "print(X_train_step_mi.shape, X_test_step_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(step_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could see that MI provide more words that describe time such as hours, minutes, 30, 350 and 45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.40335\n",
      "x2 acc 0.650025\n",
      "mi acc 0.5815\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.751825\n",
      "x2 acc 0.711875\n",
      "mi acc 0.716325\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.6481\n",
      "x2 acc 0.6481\n",
      "mi acc 0.6481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_step,y_train)\n",
    "X_train_step_x2 = x2.transform(X_train_step)\n",
    "X_test_step_x2 = x2.transform(X_test_step)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_step,y_train)\n",
    "X_train_step_mi = mi.transform(X_train_step)\n",
    "X_test_step_mi = mi.transform(X_test_step)\n",
    "\n",
    "# Chi-square -> (X_train_step_x2, X_test_step_x2)\n",
    "# Mutual Information -> (X_train_step_mi, X_test_step_mi) \n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_step, X_test_step), (X_train_step_x2, X_test_step_x2), (X_train_step_mi, X_test_step_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature step in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from name column using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ingr_train = train_df['ingredients'].values # Create an array of name for training set\n",
    "ingr_test = test_df['ingredients'].values # Create an array of name for testing set\n",
    "X_train_ingr = ingr_vectorizer.fit_transform(ingr_train)\n",
    "X_test_ingr = ingr_vectorizer.transform(ingr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2906) (10000, 2906)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ingr.shape, X_test_ingr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baking\n",
      "beef\n",
      "butter\n",
      "chicken\n",
      "eggs\n",
      "flour\n",
      "potatoes\n",
      "roast\n",
      "soup\n",
      "stew\n"
     ]
    }
   ],
   "source": [
    "# Choose best attributes\n",
    "# Find out what the best 10 features were for your name dataset, according to  ùúí2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "X_train_ingr_x2 = x2.fit_transform(X_train_ingr,y_train) # Create a sparse matrix for CountVectorizer\n",
    "X_test_ingr_x2 = x2.transform(X_test_ingr)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(ingr_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ingredients colum, we could observe that both the cooking materials and cooking methods are selected as among the best attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (10000, 10)\n",
      "baking\n",
      "beef\n",
      "butter\n",
      "eggs\n",
      "flour\n",
      "onion\n",
      "potatoes\n",
      "roast\n",
      "salt\n",
      "sugar\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing for Mutual Information, instead of  ùúí2 (note that you want the classification version, not the regression version).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_ingr_mi = mi.fit_transform(X_train_ingr,y_train)\n",
    "X_test_ingr_mi = mi.transform(X_test_ingr)\n",
    "\n",
    "print(X_train_ingr_mi.shape, X_test_ingr_mi.shape)\n",
    "\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(ingr_vectorizer.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to chi-square. Both cooking methods and cooking materials are among the best attribites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNB (with k= 1000 features):\n",
      "complete acc 0.189275\n",
      "x2 acc 0.51075\n",
      "mi acc 0.23255\n",
      "\n",
      " MNB (with k= 1000 features):\n",
      "complete acc 0.638575\n",
      "x2 acc 0.627375\n",
      "mi acc 0.62705\n",
      "\n",
      " one-r (with k= 1000 features):\n",
      "complete acc 0.537825\n",
      "x2 acc 0.537825\n",
      "mi acc 0.537825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Models to train on \n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1)]\n",
    "#          KNeighborsClassifier(n_neighbors=1),\n",
    "#          KNeighborsClassifier(n_neighbors=5),\n",
    "#          DecisionTreeClassifier(max_depth=None)]\n",
    "#          svm.LinearSVC(C=C),\n",
    "#          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "#          svm.SVC(kernel='poly', degree=3, C=C)]\n",
    "\n",
    "# Model Titles\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r']\n",
    "#          '1-nearest neighbour',\n",
    "#          '5-nearest neighbour',\n",
    "#          'Decision Tree']\n",
    "#          'LinearSVC',\n",
    "#          'SVM with a cubic kernel',\n",
    "#          'SVM with an RBF kernel']\n",
    "\n",
    "# Select Number of Features\n",
    "k = 1000\n",
    "\n",
    "# Chi-square\n",
    "x2 = SelectKBest(chi2, k=k)\n",
    "x2.fit(X_train_ingr,y_train)\n",
    "X_train_ingr_x2 = x2.transform(X_train_ingr)\n",
    "X_test_ingr_x2 = x2.transform(X_test_ingr)\n",
    "\n",
    "# Mutual Information \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "mi.fit(X_train_ingr,y_train)\n",
    "X_train_ingr_mi = mi.transform(X_train_ingr)\n",
    "X_test_ingr_mi = mi.transform(X_test_ingr)\n",
    "\n",
    "# Chi-square -> (X_train_ingr_x2, X_test_ingr_x2)\n",
    "# Mutual Information -> (X_train_ingr_mi, X_test_ingr_mi)\n",
    "\n",
    "# Fit the model and test the model \n",
    "Xs = [(X_train_ingr, X_test_ingr), (X_train_ingr_x2, X_test_ingr_x2) , (X_train_ingr_mi, X_test_ingr_mi)]\n",
    "X_names = ['complete', 'x2', 'mi']\n",
    "for title, model in zip(titles, models):\n",
    "    print('\\n',title, '(with k=',k,'features):')\n",
    "    for X_name, X in zip(X_names, Xs):\n",
    "        X_train_t, X_test_t = X\n",
    "        # convert the variable into a matrix and train it with the selected model \n",
    "        model.fit(X_train_t.todense(), y_train)\n",
    "        acc = model.score(X_train_t.todense(), y_train)\n",
    "        print(X_name, 'acc',  acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could observe that Multinomial Naive Bayes is the best model and Mutual Information is the best feature extraction method for the feature ingredients in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Training Features for CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use columns with string\n",
    "new_train_df = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sparse matrix for name\n",
    "mi_matrix = X_train_name_mi.todense()\n",
    "# Convert matrix to list\n",
    "mi_list = mi_matrix.tolist()\n",
    "# Convert list to dataframe\n",
    "mi_df = pd.DataFrame(mi_list)\n",
    "mi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of name in training set\n",
    "# mi_df.to_csv('mi_train_name_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1002)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the name features to the training set\n",
    "join_train_df = X_train.join(mi_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_matrix = X_train_step_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_step_df = pd.DataFrame(mi_list)\n",
    "\n",
    "mi_step_df = mi_step_df.add_suffix('step')\n",
    "mi_step_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in training set\n",
    "# mi_step_df.to_csv('mi_train_step_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2002)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_train_df = join_train_df.join(mi_step_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_matrix = X_train_ingr_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_ingr_df = pd.DataFrame(mi_list)\n",
    "\n",
    "mi_ingr_df = mi_ingr_df.add_suffix('ingr')\n",
    "\n",
    "mi_ingr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in training set\n",
    "# mi_ingr_df.to_csv('mi_train_ingr_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3002)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_train_df = join_train_df.join(mi_ingr_df)\n",
    "join_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Bag of Words in training set\n",
    "# join_train_df.to_csv('join_train_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Test Features For CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_matrix = X_test_name_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_name_df = pd.DataFrame(mi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of name in testing set\n",
    "# mi_name_df.to_csv('mi_test_name_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1002)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = X_test.join(mi_name_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI for step\n",
    "mi_matrix = X_test_step_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_step_df = pd.DataFrame(mi_list)\n",
    "mi_step_df = mi_step_df.add_suffix('step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of steps in testing set\n",
    "# mi_step_df.to_csv('mi_test_step_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2002)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = join_test_df.join(mi_step_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI for ingr\n",
    "mi_matrix = X_test_ingr_mi.todense()\n",
    "mi_list = mi_matrix.tolist()\n",
    "\n",
    "mi_ingr_df = pd.DataFrame(mi_list)\n",
    "mi_ingr_df = mi_ingr_df.add_suffix('ingr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bag of Words of ingr in testing set\n",
    "# mi_ingr_df.to_csv('mi_test_ingr_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3002)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_df = join_test_df.join(mi_ingr_df)\n",
    "join_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Bag of Words in testing set\n",
    "# join_test_df.to_csv('join_test_countvec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-46-d7201a8039fa>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-d7201a8039fa>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    ''''\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          #KNeighborsClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          #'KNN',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    model.fit(join_train_df,y_train)\n",
    "    acc = model.score(join_train_df, y_train)\n",
    "    print(title, \"Accuracy:\",acc)\n",
    "''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Holdout or Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold out Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(join_train_df, y_train, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split,y_train_split)\n",
    "acc = lgr.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(join_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79975\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(cross_val_score(lgr, join_train_df, y_train, cv=5))\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Holdout and Cross-validation get similar accuracy score. However, holdout is significantly faster than cross validation and for this reason, holdout strategy will be chosen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36984848484848487\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_split, y_train_split)\n",
    "acc = gnb.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_split, y_train_split)\n",
    "acc = mnb.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6552272727272728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "one_r =DecisionTreeClassifier(max_depth=1)\n",
    "one_r.fit(X_train_split, y_train_split)\n",
    "acc = one_r.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7320454545454546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_split, y_train_split)\n",
    "acc = dt.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(join_train_df, y_train, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_split,y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7999242424242424\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_split, y_test_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [#DummyClassifier(strategy='most_frequent'),\n",
    "                LogisticRegression(),\n",
    "                #KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                MultinomialNB(),\n",
    "                DecisionTreeClassifier()]\n",
    "titles = [#'Zero_R',\n",
    "          'Logistic Regression',\n",
    "          #'KNN',\n",
    "          'Gaussian NB',  \n",
    "          'Multinomial NB',\n",
    "          'Decision Tree']\n",
    "\n",
    "\n",
    "\n",
    "meta_classifier_lr = LogisticRegression()\n",
    "stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacker Accuracy (Logistic Regression): 0.7303787878787878\n"
     ]
    }
   ],
   "source": [
    "stacker_lr.fit(X_train_split, y_train_split)\n",
    "print('\\nStacker Accuracy (Logistic Regression):', stacker_lr.score(X_test_split, y_test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar = lgr.predict(join_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Doc2Vec data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Dataset \n",
    "train_name_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "test_name_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/test_name_doc2vec100.csv\", header=None)\n",
    "train_steps_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "test_steps_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/test_steps_doc2vec100.csv\", header=None)\n",
    "train_ingr_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "test_ingr_doc2vec100 = pd.read_csv(\"/Users/hesterlim/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/test_ingr_doc2vec100.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100) (40000, 100) (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_name_doc2vec100.shape, train_steps_doc2vec100.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Training Features for Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 302)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_name_doc2vec100 = train_name_doc2vec100.add_suffix('name_doc2vec')\n",
    "join_train_doc2vec100 = X_train.join(train_name_doc2vec100)\n",
    "train_steps_doc2vec100 = train_steps_doc2vec100.add_suffix('step_doc2vec')\n",
    "join_train_doc2vec100 = join_train_doc2vec100.join(train_steps_doc2vec100)\n",
    "train_ingr_doc2vec100 = train_ingr_doc2vec100.add_suffix('ingr_doc2vec')\n",
    "join_train_doc2vec100 = join_train_doc2vec100.join(train_ingr_doc2vec100)\n",
    "join_train_doc2vec100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_doc2vec100_split, X_test_doc2vec100_split, y_train_doc2vec100_split, y_test_doc2vec100_split = train_test_split(join_train_doc2vec100, y_train, test_size=0.33, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train_doc2vec100_split, y_train_doc2vec100_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "acc = lgr.score(X_test_doc2vec100_split, y_test_doc2vec100_split)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy trained using Doc2Vec is 0.727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network on X_train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_train_split_neural = X_train_split.drop(columns = ['n_steps', 'n_ingredients'])\n",
    "X_test_split_neural = X_test_split.drop(columns = ['n_steps', 'n_ingredients'])\n",
    "\n",
    "# Create Scalar\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Transform the feature\n",
    "X_train_split_neural = scaler.fit_transform(X_train_split_neural)\n",
    "X_test_split_neural = scaler.fit_transform(X_test_split_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of featuers we want\n",
    "number_of_features = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_split_neural = y_train_split.values - 1\n",
    "y_test_split_neural = y_test_split.values - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target vector to create a target matrix\n",
    "y_train_split_cat = to_categorical(y_train_split_neural)\n",
    "y_test_split_cat = to_categorical(y_test_split_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26800, 3)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_split_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=300,\n",
    "                        activation = \"relu\",\n",
    "                        input_shape=(number_of_features,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=300, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", #Cross-entropy\n",
    "               optimizer = \"rmsprop\", #Root Mean Square Propagation\n",
    "               metrics=[\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26800 samples, validate on 13200 samples\n",
      "Epoch 1/3\n",
      "26800/26800 [==============================] - 8s 299us/step - loss: nan - accuracy: 0.4442 - val_loss: nan - val_accuracy: 0.4405\n",
      "Epoch 2/3\n",
      " 9100/26800 [=========>....................] - ETA: 6s - loss: nan - accuracy: 0.4451"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-389-73e9729f4280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#No output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#Number of observations per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                      validation_data=(X_test_split_neural, y_test_split_cat)) # test data\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(X_train_split_neural, #Features\n",
    "                     y_train_split_cat, #Target\n",
    "                     epochs=3, #Three epochs\n",
    "                     verbose=1, #No output\n",
    "                     batch_size=100, #Number of observations per batch\n",
    "                     validation_data=(X_test_split_neural, y_test_split_cat)) # test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3302)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join_train_doc2vec100, join_train_df\n",
    "join_train_all = join_train_df.drop(columns = ['n_steps', 'n_ingredients'])\n",
    "join_train_all = join_train_all.join(join_train_doc2vec100)\n",
    "join_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3302)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_test_all = join_test_df.drop(columns = ['n_steps', 'n_ingredients'])\n",
    "join_test_all = join_test_all.join(join_train_doc2vec100)\n",
    "join_test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83175\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(join_all, y_train)\n",
    "acc = lgr.score(join_train_all, y_train)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar = lgr.predict(join_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df.index\n",
    "data = {'id': test_id+1, 'duration_label': ybar}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
